{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaefner/M3AAWG_AI_Training_Phishing/blob/main/Phish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx3dJMmwCZPW"
      },
      "source": [
        "# Contents\n",
        "\n",
        "1. [Loading Data](#loading_data)\n",
        "2. Exploring the Data\n",
        "3. Pre-process Data\n",
        "4.  \n",
        "\n",
        "[K-Nearest Neighbors] (#knn)\n",
        "\n",
        "[Deep Neural Networks](#dnn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k9F_4q4w2fH"
      },
      "source": [
        "First we need a dataset to work on.  The one we'll be using is from 2021 on Kagle at the URL below.\n",
        "\n",
        "\n",
        "https://www.kaggle.com/datasets/shashwatwork/web-page-phishing-detection-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB7LByUy-1cu"
      },
      "source": [
        "Next we'll start with a library that can load data from a comma dilimited file and a library used for matrix calculations.\n",
        "THe file is host on the github site.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2rsicqH3bVmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR4KMx4WEkwC"
      },
      "outputs": [],
      "source": [
        "#This hides some of the warnings we get in MLP\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "from termcolor import colored\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "import pandas as pd  #Pandas is a data manipulation library\n",
        "import numpy as np   # numpy is computing library that uses C libraries in the backend\n",
        "from sklearn.model_selection import StratifiedKFold   #This gives us nice 'slices' of examples for training and testing\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier  # K Nearest Neighbors\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decision Trees\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forrest Classifier\n",
        "from sklearn.neural_network import MLPClassifier   #Neural Network Classifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score  #Libraries for calculating scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HccFDs12tc65"
      },
      "source": [
        "<a id='loading_data'></a>\n",
        "# Loading Data\n",
        "The Data we are going to use is from a dataset hosted on Kaggle.\n",
        "\n",
        "Here:[Phishing Dataset](https://www.kaggle.com/datasets/shashwatwork/web-page-phishing-detection-dataset)\n",
        "\n",
        "Original Source of data:\n",
        "\n",
        "[Web Page Phishing Detection](#https://data.mendeley.com/datasets/c2gw7fy2j4/3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNaOkM3ktbPc"
      },
      "outputs": [],
      "source": [
        "phishing_data = pd.read_csv(\"https://raw.githubusercontent.com/khaefner/M3AAWG_AI_Training_Phishing/main/dataset_phishing.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZjSG-rXuuXV"
      },
      "source": [
        "# Exploring Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nre9rTHFEuHk"
      },
      "outputs": [],
      "source": [
        "print(phishing_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etPcc9-G95vF"
      },
      "source": [
        "We see that this data has 89 columns.  These are called *features*. In this data things like length of the url,  lenght of the hostname, etc.  Rows are datapoints corresponding to one of the domains.  These are also called *examples*.  \n",
        "\n",
        "Note:  There is one column that has special meaning.  This is the last column in the table above called, *status*.  This is the label for the website.  We are going to do **supervised** learning which means the algorithm is going to learn from the data and the label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCntFECHvC32"
      },
      "source": [
        "# Data Pre-Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVQdi8NuUO2d"
      },
      "source": [
        "Next we need to clean up the data a bit and get it ready to analyze.  \n",
        "\n",
        "\n",
        "1.   Remove the URL column.  The actual URL is not useful to the model.\n",
        "2.   Alter the Label (status=legitimate or status=phishing to 0 or 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNHrIK7QUtN-"
      },
      "outputs": [],
      "source": [
        "#Get rid of the first column:\n",
        "phishing_data = phishing_data.iloc[:, 1:]\n",
        "#Print the result\n",
        "print(phishing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7Ok5-GcU-Qg"
      },
      "outputs": [],
      "source": [
        "#Change the label classes to a one or a zero\n",
        "phishing_data['status'] = phishing_data['status'].replace({'legitimate': 0, 'phishing': 1})\n",
        "#Print the result\n",
        "phishing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTCz3eM6Vgcl"
      },
      "source": [
        "The next thing we need to do is seperate the dataset into two parts.  The labels and the examples.\n",
        "\n",
        "Typically Labels are numbers.  Here we have two classes of data:\n",
        "\n",
        "1 = Phishing site\n",
        "\n",
        "0 = Not Phishing Site"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QfJhF6-ZX0A"
      },
      "outputs": [],
      "source": [
        "#First the Labels\n",
        "y = phishing_data[\"status\"].values\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIOvvUiQY95e"
      },
      "outputs": [],
      "source": [
        "#Second the example data\n",
        "X = phishing_data.drop(\"status\", axis=1).values\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCy-iVGWAlpG"
      },
      "outputs": [],
      "source": [
        "#Let's see how many of the data are phishing and not phishing\n",
        "print(phishing_data['status'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtU9cFt7AdbL"
      },
      "source": [
        "Great we have a balance dataset.  Equal represebtation of each label phish and not phish.    Now lets look at how the features relate to each other.  There are two things we can look at **Covariance** and **Correlation**.   \n",
        "\n",
        "---\n",
        "\n",
        "Covariance:  measures how two variables (features) vary with respect to each other.  For example an increase in a person's height corresponds to an increas in a persons weight.  This would be a positive covariance.\n",
        "\n",
        "---\n",
        "Correlation: Correlation is a normalized covariance value.  What this means is that it is not affected by changes in scales.  Correlation makes the comparison measure fall between -1 and 1.    In this case a value of +1 indicates that the features have a direct and strong relationship.  Conversely a value of -1 means that the values have strong independence from one another.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2biLQprCEKq"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = phishing_data.corr(numeric_only=True)\n",
        "sorted_corr = correlation_matrix.sort_values(by='status',ascending=False)\n",
        "\n",
        "print(sorted_corr['status'].head(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZojgh7hEb7u"
      },
      "source": [
        "As we can see above, the status (label) has a 100% correlation with the outcome.  This is what we would expect.  The other features are ranked by their correlation to the decision.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSpTV80wRHO"
      },
      "source": [
        "# K Nearest Neighbors\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple machine learning algorithm that helps us make predictions based on similarity. Imagine you have a bunch of points on a graph, each with a label (like red or blue). KNN works by finding the K nearest points to a new, unlabeled point you want to classify. It then looks at the labels of those nearest points and decides the label for the new point based on majority rule. For example, if most of the nearest points are red, the new point would be classified as red. K is a number you choose, and it determines how many neighbors to consider. KNN is like asking your closest friends for advice – if most of them agree, you'll probably follow their suggestion.\n",
        "\n",
        "![image.png](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/330px-KnnClassification.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_QV9iZSZ3Ux"
      },
      "outputs": [],
      "source": [
        "\n",
        "def KNN(X,y):\n",
        "  skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "  accuracy = []\n",
        "  precision = []\n",
        "  recall = []\n",
        "  f1 = []\n",
        "\n",
        "\n",
        "  # we are going to run the model 10 times 'n_splits=10' each time we shuffle the data randomly.\n",
        "  #This helps prevent our model from overfitting.\n",
        "  for train, test in skf.split(X,y):\n",
        "     X_train, y_train = X[train], y[train] #training\n",
        "     X_test, y_test = X[test], y[test] #testing\n",
        "\n",
        "     knn = KNeighborsClassifier(n_neighbors=3)\n",
        "     knn.fit(X_train, y_train)\n",
        "\n",
        "     y_pred = knn.predict(X_test)\n",
        "\n",
        "     accuracy.append(accuracy_score(y_test, y_pred))\n",
        "     recall.append(recall_score(y_test, y_pred, average='macro'))\n",
        "     precision.append(precision_score(y_test, y_pred, average='macro'))\n",
        "     f1.append(f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "  average_accuracy = np.mean(accuracy)\n",
        "  average_recall = np.mean(recall)\n",
        "  average_precision = np.mean(precision)\n",
        "  average_f1 = np.mean(f1)\n",
        "\n",
        "  print(f\"Acurracy: {average_accuracy}\")\n",
        "  print(f\"Recall: {average_recall}\")\n",
        "  print(f\"Precision:{average_precision}\")\n",
        "  print(f\"F1 Score:{average_f1}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wY5bWg1cn2I"
      },
      "outputs": [],
      "source": [
        "KNN(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ38J5NT0Uh7"
      },
      "source": [
        "# Decision Tree\n",
        "\n",
        "A [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree) is a supervised machine learning algorithm used for both classification and regression tasks. It works by recursively splitting the dataset into subsets based on the features that provide the best separation between classes (for classification) or the best predictive power (for regression). These splits are determined by evaluating criteria like Gini impurity or information gain for classification and mean squared error for regression. The process continues until a stopping criterion is met, such as reaching a maximum depth or having too few samples in a node. Once the tree is built, it can be used to make predictions by traversing the tree from the root node to a leaf node, which corresponds to the predicted class (in classification) or the predicted value (in regression) for the input data. Decision Trees are interpretable, which means you can easily understand the reasoning behind their predictions.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Decision-Tree-Elements.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9ILgYvj0u7E"
      },
      "outputs": [],
      "source": [
        "def DT(X,y):\n",
        "  skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "  accuracy = []\n",
        "  precision = []\n",
        "  recall = []\n",
        "  f1 = []\n",
        "\n",
        "\n",
        "  for train, test in skf.split(X,y):\n",
        "     X_train, y_train = X[train], y[train] #training\n",
        "     X_test, y_test = X[test], y[test] #testing\n",
        "\n",
        "     dt = DecisionTreeClassifier(criterion='gini') #Gini is a measure of statistical dispersion that quantifies the inequality or impurity within a set of values,\n",
        "     dt.fit(X_train, y_train)\n",
        "\n",
        "     y_pred = dt.predict(X_test)\n",
        "\n",
        "     accuracy.append(accuracy_score(y_test, y_pred))\n",
        "     recall.append(recall_score(y_test, y_pred, average='macro'))\n",
        "     precision.append(precision_score(y_test, y_pred, average='macro'))\n",
        "     f1.append(f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "  average_accuracy = np.mean(accuracy)\n",
        "  average_recall = np.mean(recall)\n",
        "  average_precision = np.mean(precision)\n",
        "  average_f1 = np.mean(f1)\n",
        "\n",
        "  print(f\"Acurracy: {average_accuracy}\")\n",
        "  print(f\"Recall: {average_recall}\")\n",
        "  print(f\"Precision:{average_precision}\")\n",
        "  print(f\"F1 Score:{average_f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r4A9PhH1VRc"
      },
      "outputs": [],
      "source": [
        "DT(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXFXBvIYxbQI"
      },
      "source": [
        "# Random Forrest\n",
        "Random Forest is an ensemble learning technique in machine learning that leverages a collection of decision trees to improve predictive accuracy and reduce overfitting. It works by creating multiple decision trees during training, where each tree is constructed using a random subset of the training data and a random subset of the features. When making predictions, each tree provides its individual prediction, and the final prediction is determined by taking a majority vote (classification) or averaging (regression) across all the individual tree predictions. This ensemble approach helps enhance the robustness and generalization of the model, as it combines the strengths of multiple decision trees while mitigating their individual weaknesses and biases.\n",
        "\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Random_forest_diagram_complete.png/330px-Random_forest_diagram_complete.png\" alt=\"drawing\" width=\"50%\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6emnhNIfDZu"
      },
      "outputs": [],
      "source": [
        "#Random Forrest\n",
        "def RF(X,y):\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    accuracy = []\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "\n",
        "\n",
        "    # we are going to run the model 10 times 'n_splits=10' each time we shuffle the data randomly.\n",
        "    #This helps prevent our model from overfitting.\n",
        "    for train, test in skf.split(X,y):\n",
        "      X_train, y_train = X[train], y[train] #training\n",
        "      X_test, y_test = X[test], y[test] #testing\n",
        "\n",
        "\n",
        "      # Create a Random Forest classifier\n",
        "      rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "      # Fit the classifier to the training data\n",
        "      rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "      # Make predictions on the test data\n",
        "      y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "      accuracy.append(accuracy_score(y_test, y_pred))\n",
        "      recall.append(recall_score(y_test, y_pred, average='macro'))\n",
        "      precision.append(precision_score(y_test, y_pred, average='macro'))\n",
        "      f1.append(f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "    average_accuracy = np.mean(accuracy)\n",
        "    average_recall = np.mean(recall)\n",
        "    average_precision = np.mean(precision)\n",
        "    average_f1 = np.mean(f1)\n",
        "\n",
        "    print(f\"Acurracy: {average_accuracy}\")\n",
        "    print(f\"Recall: {average_recall}\")\n",
        "    print(f\"Precision:{average_precision}\")\n",
        "    print(f\"F1 Score:{average_f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekgFf1uCghOW"
      },
      "outputs": [],
      "source": [
        "RF(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO-3KE7Es1CE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZDo9KpsqM60"
      },
      "source": [
        "<a name=\"dnn\"></a>\n",
        "# Deep Neural Networks (DNNs)\n",
        "DNNs take the primitive perceptron and build complex networks of interconnected neurons sometimes with many hidden layers.\n",
        "\n",
        "<table width=\"100%\">\n",
        "<tr>\n",
        "<td>\n",
        "  <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Perceptron_work.png\" alt=\"drawing\"/>\n",
        "</td>\n",
        "<td>\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/MultiLayerPerceptron.svg/2560px-MultiLayerPerceptron.svg.png\" alt=\"drawing\" />\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ljeGp3sY9FO"
      },
      "outputs": [],
      "source": [
        "#Neural Nets  We'll use the Multi-Layer Perceptron MLP\n",
        "#Important hyper-paramters are Learning_rate  This affects how fast the algorihms converges (minimizes errors).\n",
        "#A learning rate that is to high will lead to sub-optimal solutions, too low and it will take forever.\n",
        "def MLP(X,y):\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    accuracy = []\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "\n",
        "\n",
        "    # we are going to run the model 10 times 'n_splits=10' each time we shuffle the data randomly.\n",
        "    #This helps prevent our model from overfitting.\n",
        "    for train, test in skf.split(X,y):\n",
        "      X_train, y_train = X[train], y[train] #training\n",
        "      X_test, y_test = X[test], y[test] #testing\n",
        "\n",
        "      mlp = MLPClassifier(\n",
        "            solver='adam',            #‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
        "            hidden_layer_sizes=(20,),\n",
        "            activation='tanh',\n",
        "            max_iter=20,\n",
        "            validation_fraction=0.2,\n",
        "            learning_rate_init=0.01,   #The amount that the weights are updated a small positive value, often in the range between 0.0 and 1.0.\n",
        "        )\n",
        "\n",
        "      # Fit (train) the classifier\n",
        "      mlp.fit(X_train,y_train)\n",
        "\n",
        "      #Predict the results using the set-aside test data.\n",
        "      y_pred = mlp.predict(X_test)\n",
        "\n",
        "      accuracy.append(accuracy_score(y_test, y_pred))\n",
        "      recall.append(recall_score(y_test, y_pred, average='macro'))\n",
        "      precision.append(precision_score(y_test, y_pred, average='macro'))\n",
        "      f1.append(f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "    average_accuracy = np.mean(accuracy)\n",
        "    average_recall = np.mean(recall)\n",
        "    average_precision = np.mean(precision)\n",
        "    average_f1 = np.mean(f1)\n",
        "\n",
        "    print(f\"Acurracy: {average_accuracy}\")\n",
        "    print(f\"Recall: {average_recall}\")\n",
        "    print(f\"Precision:{average_precision}\")\n",
        "    print(f\"F1 Score:{average_f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X4OdTs1HsEb"
      },
      "outputs": [],
      "source": [
        "MLP(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection.  \n",
        "Not all features are equal.  Remember that some of them have a higher correlation to the classifier than others.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HeeQefxJWS-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_ten_list = sorted_corr['status'].head(10)\n",
        "print(top_ten)\n",
        "X_orig = phishing_data.drop(\"status\", axis=1)\n",
        "top_ten=sorted_corr[1:11].index\n",
        "print(top_ten)"
      ],
      "metadata": {
        "id": "eSAMsMUrXSXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prime = X_orig[top_ten].values\n",
        "print(\"----------------\")\n",
        "print(\"KNN All Features\")\n",
        "print(\"________________\")\n",
        "KNN(X,y)\n",
        "print(\"----------------\")\n",
        "print(\"KNN Selected Features\")\n",
        "print(\"________________\")\n",
        "KNN(X_prime,y)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"----------------\")\n",
        "print(\"DT All Features\")\n",
        "print(\"________________\")\n",
        "DT(X,y)\n",
        "print(\"----------------\")\n",
        "print(\"DT Selected Features\")\n",
        "print(\"________________\")\n",
        "DT(X_prime,y)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"----------------\")\n",
        "print(\"RF All Features\")\n",
        "print(\"________________\")\n",
        "RF(X,y)\n",
        "print(\"----------------\")\n",
        "print(\"RF Selected Features\")\n",
        "print(\"________________\")\n",
        "RF(X_prime,y)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"----------------\")\n",
        "print(\"MLP All Features\")\n",
        "print(\"________________\")\n",
        "MLP(X,y)\n",
        "print(\"----------------\")\n",
        "print(\"MLP Selected Features\")\n",
        "print(\"________________\")\n",
        "MLP(X_prime,y)\n"
      ],
      "metadata": {
        "id": "Xcl0upVyYYN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surprised?\n",
        "A lower correlation among ensemble model members will increase the error-correcting capability of the model. So it is preferred to use models with low correlations when creating ensembles."
      ],
      "metadata": {
        "id": "8JkapIMIecQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFdEfWIoiUdK",
        "outputId": "d7dcf30f-9486-49b9-d3a8-5f34b72c8efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File integrity passed: data/url_features.py\n",
            "File downloaded to data/url_features.py\n",
            "File integrity failed: data/allbrands.txt\n",
            "File integrity failed: data/content_features.py\n",
            "File integrity passed: data/external_features.py\n",
            "File downloaded to data/external_features.py\n",
            "File integrity failed: data/feature_extractor.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from joblib.numpy_pickle import load\n",
        "import requests\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "timeout_seconds = 10\n",
        "\n",
        "#create the scratch directory for our imported python files\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.makedirs(\"data\")\n",
        "\n",
        "#This is to load files from the data directory\n",
        "init_file_path = os.path.join(\"data\", \"__init__.py\")\n",
        "\n",
        "# Create an empty __init__.py file\n",
        "with open(init_file_path, \"w\") as init_file:\n",
        "    pass\n",
        "\n",
        "#Data extraction scripts are hosted here: https://data.mendeley.com/datasets/c2gw7fy2j4/3\n",
        "# I have them on a github page as I needed to remove some NLP depenadancies\n",
        "all_brands_url = \"https://raw.githubusercontent.com/khaefner/M3AAWG_AI_Training_Phishing/main/allbrands.txt\"\n",
        "all_brands_sha = \"58fc066042181abbb1b42dd9ebf046dd0826347f93a6c8a6c129a4b8fb252efe\"\n",
        "\n",
        "url_features_url =\"https://raw.githubusercontent.com/khaefner/M3AAWG_AI_Training_Phishing/main/url_features.py\"\n",
        "url_features_sha=\"6c36b9db9518f8e2bf12d1cc2b5eae3ef88fe7f517280792d9895af73028c78b\"\n",
        "\n",
        "content_features_url = \"https://raw.githubusercontent.com/khaefner/M3AAWG_AI_Training_Phishing/main/content_features.py\"\n",
        "content_features_sha = \"3165d2aa24322bb8db79f59070b4b4930661ad4afc54d56cf915261e06bc9d28\"\n",
        "\n",
        "external_features_url = \"https://raw.githubusercontent.com/khaefner/M3AAWG_AI_Training_Phishing/main/external_features.py\"\n",
        "external_features_sha = \"b4a0b2147163cf0c12d66e2392d443f4b1b131de3b84a3b9403d2f7ed00171cb\"\n",
        "\n",
        "feature_extractor_url = \"https://raw.githubusercontent.com/khaefner/M3AAWG_AI_Training_Phishing/main/feature_extractor.py\"\n",
        "feature_extractor_sha = \"4638d1d578fd80be7e2adae1917280e39d0c2906d8db1e3a44ae571bb5e8a317\"\n",
        "\n",
        "\n",
        "\n",
        "def calculate_hash(file_path,expected_hash):\n",
        "  # Create a SHA-256 hash object\n",
        "  sha256 = hashlib.sha256()\n",
        "\n",
        "  # Read the file in binary mode and update the hash object\n",
        "  with open(file_path, \"rb\") as file:\n",
        "      while True:\n",
        "          data = file.read(65536)  # You can adjust the buffer size as needed\n",
        "          if not data:\n",
        "              break\n",
        "          sha256.update(data)\n",
        "  calculated_hash = sha256.hexdigest()\n",
        "  if calculated_hash == expected_hash:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_lib(url, destination_path, expected_hash):\n",
        "  try:\n",
        "    response = requests.get(url, timeout=timeout_seconds)\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Open the local file in binary write mode and write the content of the response to it\n",
        "        with open(\"data/\"+destination_path, \"wb\") as file:\n",
        "          file.write(response.content)\n",
        "          if calculate_hash(\"data/\"+destination_path,expected_hash):\n",
        "              print(f\"File integrity passed: {'data/'+destination_path}\")\n",
        "          else:\n",
        "              print(f\"File integrity failed: {'data/'+destination_path}\")\n",
        "              return\n",
        "        print(f\"File downloaded to {'data/'+destination_path}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
        "  except requests.exceptions.Timeout:\n",
        "    print(f\"Request timed out after {timeout_seconds} seconds.\")\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred during the request: {str(e)}\")\n",
        "\n",
        "get_lib(url_features_url,\"url_features.py\",url_features_sha)\n",
        "get_lib(all_brands_url,\"allbrands.txt\",all_brands_sha)\n",
        "get_lib(content_features_url,\"content_features.py\",content_features_sha)\n",
        "get_lib(external_features_url,\"external_features.py\",external_features_sha)\n",
        "get_lib(feature_extractor_url,\"feature_extractor.py\",feature_extractor_sha)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets immport some of the code to analyze new websites.\n",
        "!pip install Levenshtein\n",
        "!pip install whois\n",
        "!pip install dnspython\n",
        "!pip install tldextract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLE6e-W84WMh",
        "outputId": "a90c6b42-b9e1-4b39-bd45-ac0b685109a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.9/172.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.22.0 rapidfuzz-3.3.1\n",
            "Collecting whois\n",
            "  Downloading whois-0.9.27.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: whois\n",
            "  Building wheel for whois (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whois: filename=whois-0.9.27-py3-none-any.whl size=30468 sha256=fa5b838462402fa0ff70dbac3ec187e76a5e985316351b2edb87823a1a185332\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/17/36/d62eb5bcc416650499a7259d584c11e4a778de5ce0e72a8dbf\n",
            "Successfully built whois\n",
            "Installing collected packages: whois\n",
            "Successfully installed whois-0.9.27\n",
            "Collecting dnspython\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython\n",
            "Successfully installed dnspython-2.4.2\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.6.0-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.12.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file>=1.4->tldextract) (1.16.0)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-1.5.1 tldextract-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import data.url_features\n",
        "import data.content_features\n",
        "import data.external_features\n",
        "import data.feature_extractor as fextract2"
      ],
      "metadata": {
        "id": "j-eYxzP6ojmV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://cablelabs.com\"\n",
        "fextract2.extract_features(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAcwdAY9ZTTV",
        "outputId": "08266746-9b51-4b44-9290-ab1f4bc00e84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domain cablelabs.com\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://cablelabs.com',\n",
              " 21,\n",
              " 13,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 9,\n",
              " 9,\n",
              " 0,\n",
              " 9,\n",
              " 9,\n",
              " 0,\n",
              " 9.0,\n",
              " 9.0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 115,\n",
              " 0.6434782608695652,\n",
              " 0.3565217391304348,\n",
              " 0.0,\n",
              " 0,\n",
              " 0.0,\n",
              " 0.43902439024390244,\n",
              " 0.0,\n",
              " 0.21951219512195122,\n",
              " 0,\n",
              " 1,\n",
              " 20.0,\n",
              " 0,\n",
              " 94.11764705882352,\n",
              " 5.88235294117647,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " None]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}